{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Kung-Fu with advantage actor-critic\n",
    "\n",
    "In this notebook you'll build a deep reinforcement learning agent for Atari [Kung-Fu Master](https://gym.openai.com/envs/KungFuMaster-v0/) and train it with Advantage Actor-Critic.\n",
    "\n",
    "![https://upload.wikimedia.org/wikipedia/en/6/66/Kung_fu_master_mame.png](https://upload.wikimedia.org/wikipedia/en/6/66/Kung_fu_master_mame.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    %tensorflow_version 1.x\n",
    "    \n",
    "    if not os.path.exists('.setup_complete'):\n",
    "        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/spring20/setup_colab.sh -O- | bash\n",
    "\n",
    "        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/grading.py -O ../grading.py\n",
    "        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week5_policy_based/submit.py\n",
    "        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week5_policy_based/atari_util.py\n",
    "\n",
    "        !touch .setup_complete\n",
    "\n",
    "# If you are running on a server, launch xvfb to record game videos\n",
    "# Please make sure you have xvfb installed\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For starters, let's take a look at the game itself:\n",
    "\n",
    "* Image resized to 42x42 and converted to grayscale to run faster\n",
    "* Agent sees last 4 frames of game to account for object velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation shape: (42, 42, 4)\n",
      "Num actions: 14\n",
      "Action names: ['NOOP', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'DOWNRIGHT', 'DOWNLEFT', 'RIGHTFIRE', 'LEFTFIRE', 'DOWNFIRE', 'UPRIGHTFIRE', 'UPLEFTFIRE', 'DOWNRIGHTFIRE', 'DOWNLEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from atari_util import PreprocessAtari\n",
    "\n",
    "def make_env():\n",
    "    env = gym.make(\"KungFuMasterDeterministic-v0\")\n",
    "    env = PreprocessAtari(\n",
    "        env, height=42, width=42,\n",
    "        crop=lambda img: img[60:-30, 5:],\n",
    "        dim_order='tensorflow',\n",
    "        color=False, n_frames=4)\n",
    "    return env\n",
    "\n",
    "env = make_env()\n",
    "\n",
    "obs_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"Observation shape:\", obs_shape)\n",
    "print(\"Num actions:\", n_actions)\n",
    "print(\"Action names:\", env.env.env.get_action_meanings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAACDCAYAAACUaEA8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAWGUlEQVR4nO3debgcVZnH8e9Lbm72hRCWkARCICjwyL4EREUW2cKiogYRgoKIKyCiIDMIKvOAIoM+OCwuwGDYBCSZCCMMBhDFQNgSQgIJsiQQskDIQgLZ3vnjnCZ1m+57+3ZXV3ddfp/n6ed213beeqvr9KlTyzV3R0RE8mejRgcgIiLVUQUuIpJTqsBFRHJKFbiISE6pAhcRySlV4CIiOaUKXMoysxFm5mbW0uhYOsPMTjCze+u07B5m9qyZbVHl/Jub2UNmttzMfpF2fLWK23u7Kudts24WXGdmS8zs0SqX+aiZ7VTNvB8Eudox887MHgB2AbZw93czKtOBUe4+J4vysmZmI4AXge7uvhbA3ccD4+tU5GnAQ+7+elEcrcA0oK+7D+tg/sVAf+96N2G0WTcz+xhwCDDM3d+ucpmXAT8GPptSjF2KWuAZiRXNxwAHjm5oME0kttLy9D38GnBjieHnAAsrmH9r4NlylXfejnaKFK/b1sBL1VTeiTxMBD5pZkNSirFrcXe9MngBFwB/By4HJhWN2wT4H2AZ8BjwU+DhxPgPA/cBbwLPAZ9PjLse+DXwZ2A5MAXYNo57iPCD8TawAvhCibg2Av4NeJlQAf03MCCOGxHnPw14DZgPnJ2Yd29gaox7AXB5Ytxo4B/AW8DTwAGJcQ8AF8d8rIrlTy2K6yxgYnx/JPBkLGcucGFiuldijCvia1/g5KL87RfzujT+3a8olp/EWJYD9wKDy2zDrWK8LUXDtwFmAocD89r5DlwPrAFWx1gPBi4Ebgf+ENfv1JjXR2Lu5gNXAq2J5TjwDWB2jPknwLZxnmXAbUXTjwGeisv7B7BzOzE68B3gX4TW9M+BjeK4C4E/JKYtfD9aSqzb14B3gHXx80UdxQK8BPyAcCTzbiHPhO/+uEbvw834angAH5QXMCfudHvEL/rmiXG3xFdvYMdYST0cx/WJn78cd5Td4461Uxx/PaFi3zuOHw/ckli2A9u1E9dXYmwjgb7AncCNcVxhB705xvERYBFwcBz/CHBifN8XGB3fDwXeAI4g/EAcEj9vGsc/QKh4d4oxD4gV0ahEXI8BY+P7A2LZGwE7E34sji2KsSUx78mJ/A0ClgAnxrKOj583ScTyArA90Ct+vqRMro4EZpQYPgn4dIyzbAWe2F4/TXy+MH4fjo3r1yt+R0bHeEcQfhzOLNqmE4H+MYfvAvfHbTgAeJZY4RG+LwuBfYBuwDhCRdmjTHwOTI552wp4Hjg1EWvJCrzMur23HSqJJb5/ChgO9ErM9ysSjQO9NrzydOiaW2a2P+Fw8jZ3f5xQYXwxjutG6N/7kbuvdPdngRsSs48hHIZe5+5r3f0J4A7guMQ0d7r7ox76gMcDu3YivBMIO8e/3H0FcB4wtuhQ/iJ3f9vdpwPXESpBCBXPdmY22N1XuPs/4/AvAXe7+93uvt7d7yO01I9ILPN6d58R12kpMKGwXDMbRTjqmAjg7g+4+/S4rGmEH5RPVLh+RwKz3f3GWNbNwCzgqMQ017n78+6+itB6LZe/gYQfmveY2acJFdifKoynlEfc/a64fqvc/XF3/2eM9yXgGt6/vpe6+zJ3nwE8A9wbt+FS4B5gtzjdV4Fr3H2Ku69z9xsIFf7oduK51N3fdPdXgCvYsL1rVUksv3L3uXFbFCwn5F6KqALPxjjCDrY4fr4pDgPYlNDSmpuYPvl+a2AfM3ur8CJUusmrIJIn1FYSWsOV2pLQfVLwcoxn8zLxvBznATiF0HKdZWaPmdmYRMyfK4p5fyDZj5lcJoScFCqKLwJ3uftKADPbx8wmm9kiM1sKnA4MrnL9CuswNPG50vwtAfoVPphZH+BnwLdLTWxmV5vZivj6YTsxtsmFmW1vZpPM7HUzWwb8B+9f3wWJ96tKfC6sw9bA2UXbYjgbtmFH8SS3d60qiaX4ewEh52+lFEOXkucTJrlgZr2AzwPdzKxQUfQABprZLoTW01pgGOFwFcKXumAu8KC7H1KnEF8j7FgFW8V4FsSYCvHMSox/DcDdZwPHx5OQnwFuN7NNYsw3uvtX2ym3+CTevcBgM9uVUJGflRh3E6Ef+HB3f8fMrmBDhdbRlRzF61dYh//tYL5SpgEjzawlHu2MInQj/M3MAFqBAXE7j3b30wk/Nh0pXoerCH3+x7v7cjM7k7ZHXJ0xF7jY3S/uxDzDgRnx/Xvbm3AupXdius5eSllJLKW25w6EcwRSRC3w+juWcCJnR8Kh+a6EL+TfgJPcfR2h3/lCM+ttZh8GTkrMPwnY3sxONLPu8bWXme1QYfkLCH2j5dwMnGVm25hZX0Jr79ZYQRX8e4xtJ0Jf/K0AZvYlM9vU3dezoYW0jrCzHWVmh5pZNzPraWYHmFnZy+tiebcTTpoNIpy4KugHvBkr772J3U/RImB9O+t4NyF/XzSzFjP7AmFbTGonJ+VinEc4cbh3HPQMobIrbNdTCfneldItyUr1I5yMXBG/D1+vYVm/AU6PRzFmZn3M7Egz69fOPOeY2cZmNhw4g7i9Cf3THzezrcxsAKG7ra6xmFkPwjmB+8pN80GmCrz+xhH6WF9x99cLL0KL8oTY1/wtwsmn1wmXqN1M6BvE3ZcDnwLGElpCrwOXElrxlbgQuCEesn6+xPjfxzIfIlxP/Q7v7xJ4kHCi837gMncv3CRzGDDDzFYAvyScdHzH3ecCxwA/JFSwcwmX2XX0fbuJcGXGH4t+QL4B/NjMlhOu5rmtMCJ2s1wM/D2uY5u+XXd/g3Ae4WzCidTvA2MS3VmddQ3hhCixjzq5Td8E1sfP66pcPsD3CD9SywmV3q3tT16eu08l9D1fSegCmkM4udieCcDjhAr7z8Dv4rLui7FMi+M79SNYZSxHAw+4+2sdTPeBZO5d7V6C/DOzSwk3+4zrcGLJVGwRPgkc5O7zGx1PV2dmU4BT3P2ZRsfSjFSBN4F4mNwKTAf2Ihz2n+rudzU0MBFpajqJ2Rz6EbpNtiRcJ/sLwmGsiEhZNbXAzewwQt9nN+C37n5JWoGJiEj7qq7A4w0ozxPusptHuHPu+HgjioiI1FktV6HsDcyJd3+tJtwKfkw6YYmISEdq6QMfSttrXecRnnFQVqv18J70Ye2mfWooVgpaFoWHvCmftVMu06V8pmvVonmL3X3T4uG1VOBWYtj7+mPM7DTC0+zoSW/2sYNY+IX9aihWCjb79T8AlM8UKJfpUj7TNe3K7xY/DgKorQKfR9tbvoex4Zbb97j7tcC1AP1tkANseY+uyU9D4U4X5bN2ymW6lM90TSszvJY+8MeAUfEW7FbCnYITa1ieiIh0QtUtcHdfa2bfAv5CuIzw9/HRlh16YVxaDzf7YNv6Ry8BymcalMt0KZ8pu6D04Jpu5HH3uwl3DYqISMYacidm6y5LGlFsl6V8pke5TJfyWV96GqGISE41pAW+4tX+qS7P+q8GwJe11ragvmvC3xXda4yoRv1iHMsri0P57EAn8qlcdkDfzXR1Mp/F1AIXEcmpLvE0QlsY/reB96zt0bgbLQ6/6utrXE6tNlrU2DiUz/Qol+lSPovmTzMYERHJTpdogffZdikAy2vsb+u5zXIAVs7vzD91T1+vkcsAePu19v5tYf0on+lRLtOlfLalFriISE51iRb4qmcHhjcD1te0nNXPx1/1frUtp1bvzhoQ3vRvTBzKZ3qUy3Qpn22pBS4iklNdogVu24ZnD7O4V20L2npl+Ptmz9qWU6tt4vq8UeP6VEn5TI9ymS7lsy21wEVEcqpLtMDXLIq/XqX+xUQnrH2jZyrLqdXaxemsT7WUz/Qol+lSPttSC1xEJKca0gK3AasbUWyXpXymR7lMl/JZX2qBi4jkVENa4L6kxieJSRvKZ3qUy3Qpn/WlFriISE415ioU/WykS/lMj3KZLuWzrpReEZGcUgUuIpJTqsBFRHJKFbiISE6pAhcRySlV4CIiOaUKXEQkp1SBi4jklCpwEZGcUgUuIpJTqsBFRHJKFbiISE51WIGb2XAzm2xmM81shpmdEYcPMrP7zGx2/Ltx/cMVEZGCSlrga4Gz3X0HYDTwTTPbETgXuN/dRwH3x88iIpKRDitwd5/v7k/E98uBmcBQ4BjghjjZDcCx9QpSRETer1N94GY2AtgNmAJs7u7zIVTywGZpByciIuVVXIGbWV/gDuBMd1/WiflOM7OpZjZ1De9WE6OIiJRQUQVuZt0Jlfd4d78zDl5gZkPi+CHAwlLzuvu17r6nu+/ZnR5pxCwiIlR2FYoBvwNmuvvliVETgXHx/ThgQvrhiYhIOZX8T8yPAicC083sqTjsh8AlwG1mdgrwCvC5+oQoIiKldFiBu/vDgJUZfVC64YiISKVycSfmqG9OYdQ3p3TZ8rKW9fp96JzpfOic6ZmVlyXlMl3a1zsnFxW4iIi8XyV94A0z9P7w9+3j9sm03NWH7ZVpeVlpVD7Xr1wJQO+53QBYOXxdpuXXg3KZLu3r1VELXEQkp5q6Bb5obGhtbDFweRgwe/NsCj47XNJuCwYB4Etbsym3zhqVz2X3bAvAyn91jdYiKJdp075eHbXARURyqqlb4NtvvgiAMZtNA+CS2UfVtbxuG4db/SfvFO5JunzISACufPCQupablazzWfDILneEN7uEPyvWvwPAzhPOyKT8elAu06V9vTpqgYuI5FRTt8DXrg+/L9u2Lsi03CXrQn/c6QNnAXAl+fpVLifrfN515C/ju54AnPTyxwF4+IkdMim/npTLdGlfr45a4CIiOdXULfCsrY1noDfu1huAkX/6WiPDyb1j7vkOAC8ecy3QdVqLjaBcpqur7OtqgYuI5FRTt8BfnTACgAte+2oY8Amva3kty8LdbQeefGoYcExdi8tc1vm09eEZaF0xn8plurSvV0ctcBGRnGrqFvieY8M1oZcNvReAPSaeVd8Ch68C4K/XXw/A6KeOA2Dhi5vUt9yMZJ3PwrW2XTGfymW6tK9XRy1wEZGcauoW+OTHdwLgqLey+Yf365a0/Z+d52wXWgPnvHh8JuXXm/KZHuUyXcpnddQCFxHJqVxU4BeNmsBFo7L/n8mf7buMz/Zdlnm59aZ8pke5TJfy2Tm5qMBFROT9mroP/MM7zgXgHe+eSXnWfzUAox44GYDZB1yfSblZUT7To1ymS/msjlrgIiI51ZAWuLesr2i6mc8PBeC/vnJomO/cyuarVvdXwpPeVg9eC2x4PkKl8TaK8pke5TJdymd9qQUuIpJTDWmBb7SqW6emn3XGkDhfPaLZYG0fj+W0jc/WdC7erCmf6VEu06V81pda4CIiOdWQFvj6Xl3rP2o3mvKZHuUyXcpnfakFLiKSU6rARURyShW4iEhONaQPfOMnm/oG0NxRPtOjXKZL+UzHK2WGV9wCN7NuZvakmU2Kn7cxsylmNtvMbjWz1lQiFRGRinTm5/EMYCbQP36+FPhPd7/FzK4GTgGuqmRBb+2Ur7udmtWm8a/yWTvlMl3KZzYqaoGb2TDgSOC38bMBBwK3x0luAI6tR4AiIlJapS3wK4DvA/3i502At9x9bfw8DxhaaaEDZ+jcaZqUz/Qol+lSPtPxcpnhHWbXzMYAC9398eTgEpN6mflPM7OpZjZ1De92HKmIiFSkkhb4R4GjzewIoCehD/wKYKCZtcRW+DDgtVIzu/u1wLUA/W1QyUp+ye6hIf+ZPcJvxOTf7NOplSh46yOhv81Wh9+XAc+V+p0pb48vh/+M/fh1O4fljQ7PDB74z86dny3EMW7/vwEw4apPdGr+gkJeWpaEzdTvxc7Np3y2VU0+lcvS9N1sG0ej8tlhC9zdz3P3Ye4+AhgL/NXdTwAmA8fFycYB2f8fJBGRDzBzL9koLj2x2QHA99x9jJmNBG4BBgFPAl9y93b7SMxsEfA2sLjqiOtvMIqvWs0cGyi+Wim+2tQS39buvmnxwE5V4Gkws6nuvmemhXaC4qteM8cGiq9Wiq829YhPp4hFRHJKFbiISE41ogK/tgFldobiq14zxwaKr1aKrzapx5d5H7iIiKRDXSgiIjmVWQVuZoeZ2XNmNsfMzs2q3HbiGW5mk81sppnNMLMz4vBBZnZffMrifWa2cYPjbNqnQJrZQDO73cxmxTzu20z5M7Oz4rZ9xsxuNrOejcyfmf3ezBaa2TOJYSXzZcGv4v4yzcx2b1B8P4/bd5qZ/cnMBibGnRfje87MDm1EfIlx3zMzN7PB8XOm+SsXm5l9O+Znhpn9LDE8ndy5e91fQDfgBWAk0Ao8DeyYRdntxDQE2D2+7wc8D+wI/Aw4Nw4/F7i0wXF+F7gJmBQ/3waMje+vBr7ewNhuAE6N71uBgc2SP8KzeV4EeiXydnIj8wd8HNgdeCYxrGS+gCOAewiPrRgNTGlQfJ8CWuL7SxPx7Rj34x7ANnH/7pZ1fHH4cOAvhEeGDG5E/srk7pPA/wE94ufN0s5dVl/cfYG/JD6fB5yXRdmdiHECcAjwHDAkDhsCPNfAmIYB9xOe/DgpfhkXJ3aoNnnNOLb+sYK0ouFNkb9Ygc8l3GjWEvN3aKPzB4wo2slL5gu4Bji+1HRZxlc07tPA+Pi+zT4cK9B9GxEf4amouwAvJSrwzPNXYtveBhxcYrrUcpdVF0phZyro1NML683MRgC7AVOAzd19PkD8u1njInvvKZCFhyrX9BTIlI0EFgHXxS6e35pZH5okf+7+KnAZ4Z+ZzAeWAo/TPPkrKJevZtxnvkJo1UKTxGdmRwOvuvvTRaOaIb7tgY/FLrsHzWyvtGPLqgKv+OmFWTOzvsAdwJnuvqzR8RTU+hTIDLQQDhmvcvfdCI9IaPi5jYLYl3wM4RB1S6APcHiJSZvie1hCM21rzOx8YC0wvjCoxGSZxmdmvYHzgQtKjS4xLOv8tQAbE7pwzgFuMzMjxdiyqsDnEfqpCso+vTBLZtadUHmPd/c74+AFZjYkjh8CLGxQeIWnQL5EeObMgSSeAhmnaWQe5wHz3H1K/Hw7oUJvlvwdDLzo7ovcfQ1wJ7AfzZO/gnL5app9xszGAWOAEzwe89Mc8W1L+IF+Ou4nw4AnzGyLJolvHnCnB48SjqQHpxlbVhX4Y8CoeAVAK+GphhMzKruk+Ev4O2Cmu1+eGDWR8HRFaOBTFr3JnwLp7q8Dc83sQ3HQQcCzNEn+CF0no82sd9zWhfiaIn8J5fI1ETgpXk0xGlha6GrJkpkdBvwAONrdVyZGTQTGmlkPM9sGGAU8mmVs7j7d3Tdz9xFxP5lHuDDhdZojf3cRGl6Y2faEE/2LSTN39T7pkOioP4JwpccLwPlZldtOPPsTDlumAU/F1xGEfub7gdnx76AmiPUANlyFMjJu7DnAH4lnuBsU167A1JjDuwiHi02TP+AiYBbwDHAj4ax/w/IH3Ezoj19DqGxOKZcvwmH2r+P+Mh3Ys0HxzSH01xb2kasT058f43sOOLwR8RWNf4kNJzEzzV+Z3LUCf4jfvyeAA9POne7EFBHJKd2JKSKSU6rARURyShW4iEhOqQIXEckpVeAiIjmlClxEJKdUgYuI5JQqcBGRnPp/PDkMELU7rfsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "for _ in range(100):\n",
    "    s, _, _, _ = env.step(env.action_space.sample())\n",
    "\n",
    "plt.title('Game image')\n",
    "plt.imshow(env.render('rgb_array'))\n",
    "plt.show()\n",
    "\n",
    "plt.title('Agent observation (4-frame buffer)')\n",
    "plt.imshow(s.transpose([0, 2, 1]).reshape([42,-1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build an agent\n",
    "\n",
    "We now have to build an agent for actor-critic training — a convolutional neural network that converts states into action probabilities $\\pi$ and state values $V$.\n",
    "\n",
    "Your assignment here is to build and apply a neural network. You can use any framework you want, but in this notebook we prepared for you a template in Tensorflow.\n",
    "\n",
    "For starters, we want you to implement this architecture:\n",
    "\n",
    "![https://s17.postimg.cc/orswlfzcv/nnet_arch.png](https://s17.postimg.cc/orswlfzcv/nnet_arch.png)\n",
    "\n",
    "After your agent gets mean reward above 5000, we encourage you to experiment with model architecture to score even better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.reset_default_graph()\n",
    "sess = tf.compat.v1.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, Dense, Flatten, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, name, state_shape, n_actions, reuse=False):\n",
    "        \"\"\"A simple actor-critic agent\"\"\"\n",
    "        \n",
    "        with tf.variable_scope(name, reuse=reuse):\n",
    "            \n",
    "            # Prepare neural network architecture\n",
    "            <YOUR CODE: prepare any necessary layers, variables, etc.>\n",
    "            \n",
    "            # prepare a graph for agent step\n",
    "            self.state_t = tf.placeholder('float32', [None,] + list(state_shape))\n",
    "            self.agent_outputs = self.symbolic_step(self.state_t)\n",
    "        \n",
    "    def symbolic_step(self, state_t):\n",
    "        \"\"\"Takes agent's previous step and observation, returns next state and whatever it needs to learn (tf tensors)\"\"\"\n",
    "        \n",
    "        # Apply neural network\n",
    "        <YOUR CODE: apply the agent's neural network to get policy logits and state values>\n",
    "        \n",
    "        logits = <YOUR CODE>\n",
    "        state_values = <YOUR CODE>\n",
    "        \n",
    "        assert tf.is_numeric_tensor(logits) and logits.shape.ndims == 2, \\\n",
    "            \"Please return 2D TF tensor of logits [you got %s].\" % repr(logits)\n",
    "        assert tf.is_numeric_tensor(state_values) and state_values.shape.ndims == 1, \\\n",
    "            \"Please return 1D TF tensor of state values [you got %s].\" % repr(state_values)\n",
    "        # Hint: if you triggered state_values assert with your shape being [None, 1],\n",
    "        # just select [:, 0]-th element of state values as new state values.\n",
    "        # Alternatively, you can do tf.squeeze(state_values, axis=1).\n",
    "        \n",
    "        return logits, state_values\n",
    "    \n",
    "    def step(self, state_t):\n",
    "        \"\"\"Same as symbolic step except it operates on numpy arrays\"\"\"\n",
    "        sess = tf.get_default_session()\n",
    "        return sess.run(self.agent_outputs, {self.state_t: state_t})\n",
    "    \n",
    "    def sample_actions(self, agent_outputs):\n",
    "        \"\"\"pick actions given numeric agent outputs (np arrays)\"\"\"\n",
    "        logits, state_values = agent_outputs\n",
    "        policy = np.exp(logits) / np.sum(np.exp(logits), axis=-1, keepdims=True)\n",
    "        return np.array([np.random.choice(len(p), p=p) for p in policy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\"agent\", obs_shape, n_actions)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = [env.reset()]\n",
    "logits, value = agent.step(state)\n",
    "print(\"action logits:\\n\", logits)\n",
    "print(\"state values:\\n\", value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's play!\n",
    "Let's build a function that measures agent's average reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, env, n_games=1):\n",
    "    \"\"\"Plays an a game from start till done, returns per-game rewards \"\"\"\n",
    "\n",
    "    game_rewards = []\n",
    "    for _ in range(n_games):\n",
    "        state = env.reset()\n",
    "\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            action = agent.sample_actions(agent.step([state]))[0]\n",
    "            state, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        game_rewards.append(total_reward)\n",
    "    return game_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym.wrappers\n",
    "\n",
    "with gym.wrappers.Monitor(make_env(), directory=\"videos\", force=True) as env_monitor:\n",
    "    final_rewards = evaluate(agent, env_monitor, n_games=3)\n",
    "\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show video. This may not work in some setups. If it doesn't\n",
    "# work for you, you can download the videos and view them locally.\n",
    "\n",
    "from pathlib import Path\n",
    "from IPython.display import HTML\n",
    "\n",
    "video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(video_names[-1]))  # You can also try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on parallel games\n",
    "\n",
    "![img](https://s7.postimg.cc/4y36s2b2z/env_pool.png)\n",
    "\n",
    "\n",
    "To make actor-critic training more stable, we shall play several games in parallel. This means ya'll have to initialize several parallel gym envs, send agent's actions there and .reset() each env if it becomes terminated. To minimize learner brain damage, we've taken care of them for ya - just make sure you read it before you use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvBatch:\n",
    "    def __init__(self, n_envs = 10):\n",
    "        \"\"\" Creates n_envs environments and babysits them for ya' \"\"\"\n",
    "        self.envs = [make_env() for _ in range(n_envs)]\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\" Reset all games and return [n_envs, *obs_shape] observations \"\"\"\n",
    "        return np.array([env.reset() for env in self.envs])\n",
    "    \n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        Send a vector[batch_size] of actions into respective environments\n",
    "        :returns: observations[n_envs, *obs_shape], rewards[n_envs], done[n_envs,], info[n_envs]\n",
    "        \"\"\"\n",
    "        results = [env.step(a) for env, a in zip(self.envs, actions)]\n",
    "        new_obs, rewards, done, infos = map(np.array, zip(*results))\n",
    "        \n",
    "        # reset environments automatically\n",
    "        for i in range(len(self.envs)):\n",
    "            if done[i]:\n",
    "                new_obs[i] = self.envs[i].reset()\n",
    "        \n",
    "        return new_obs, rewards, done, infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's try it out:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_batch = EnvBatch(10)\n",
    "\n",
    "batch_states = env_batch.reset()\n",
    "\n",
    "batch_actions = agent.sample_actions(agent.step(batch_states))\n",
    "\n",
    "batch_next_states, batch_rewards, batch_done, _ = env_batch.step(batch_actions)\n",
    "\n",
    "print(\"State shape:\", batch_states.shape)\n",
    "print(\"Actions:\", batch_actions)\n",
    "print(\"Rewards:\", batch_rewards)\n",
    "print(\"Done:\", batch_done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-critic\n",
    "\n",
    "Here we define loss functions and learning algorithms as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These placeholders mean exactly the same as in \"Let's try it out\" section above\n",
    "states_ph = tf.placeholder('float32', [None,] + list(obs_shape))    \n",
    "next_states_ph = tf.placeholder('float32', [None,] + list(obs_shape))\n",
    "actions_ph = tf.placeholder('int32', (None,))\n",
    "rewards_ph = tf.placeholder('float32', (None,))\n",
    "is_done_ph = tf.placeholder('float32', (None,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits[n_envs, n_actions] and state_values[n_envs, n_actions]\n",
    "logits, state_values = agent.symbolic_step(states_ph)\n",
    "next_logits, next_state_values = agent.symbolic_step(next_states_ph)\n",
    "\n",
    "# There is no next state if the episode is done!\n",
    "next_state_values = next_state_values * (1 - is_done_ph)\n",
    "\n",
    "# probabilities and log-probabilities for all actions\n",
    "probs = tf.nn.softmax(logits, axis=-1)            # [n_envs, n_actions]\n",
    "logprobs = tf.nn.log_softmax(logits, axis=-1)     # [n_envs, n_actions]\n",
    "\n",
    "# log-probabilities only for agent's chosen actions\n",
    "logp_actions = tf.reduce_sum(logprobs * tf.one_hot(actions_ph, n_actions), axis=-1) # [n_envs,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute advantage using rewards_ph, state_values and next_state_values.\n",
    "gamma = 0.99\n",
    "advantage = <YOUR CODE>\n",
    "\n",
    "assert advantage.shape.ndims == 1, \"please compute advantage for each sample, vector of shape [n_envs,]\"\n",
    "\n",
    "# Compute policy entropy given logits_seq. Mind the \"-\" sign!\n",
    "entropy = <YOUR CODE>\n",
    "\n",
    "assert entropy.shape.ndims == 1, \"please compute pointwise entropy vector of shape [n_envs,] \"\n",
    "\n",
    "# Compute target state values using temporal difference formula. Use rewards_ph and next_step_values\n",
    "target_state_values = <YOUR CODE>\n",
    "\n",
    "\n",
    "actor_loss = -tf.reduce_mean(logp_actions * tf.stop_gradient(advantage), axis=0) - 0.001 * tf.reduce_mean(entropy, axis=0)\n",
    "critic_loss = tf.reduce_mean((state_values - tf.stop_gradient(target_state_values))**2, axis=0)\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(actor_loss + critic_loss)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks to catch some errors. Specific to KungFuMaster in assignment's default setup.\n",
    "l_act, l_crit, adv, ent = sess.run([actor_loss, critic_loss, advantage, entropy], feed_dict = {\n",
    "        states_ph: batch_states,\n",
    "        actions_ph: batch_actions,\n",
    "        next_states_ph: batch_states,\n",
    "        rewards_ph: batch_rewards,\n",
    "        is_done_ph: batch_done,\n",
    "    })\n",
    "\n",
    "assert abs(l_act) < 100 and abs(l_crit) < 100, \"losses seem abnormally large\"\n",
    "assert 0 <= ent.mean() <= np.log(n_actions), \"impossible entropy value, double-check the formula pls\"\n",
    "if ent.mean() < np.log(n_actions) / 2:\n",
    "    print(\"Entropy is too low for untrained agent\")\n",
    "print(\"You just might be fine!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train \n",
    "\n",
    "Just the usual - play a bit, compute loss, follow the graidents, repeat a few million times.\n",
    "\n",
    "![Daniel San training the Karate Kid](https://media.giphy.com/media/W4uQMqlKVoiXK89T5j/giphy.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def ewma(x, span=100):\n",
    "    return pd.DataFrame({'x':np.asarray(x)}).x.ewm(span=span).mean().values\n",
    "\n",
    "env_batch = EnvBatch(10)\n",
    "batch_states = env_batch.reset()\n",
    "\n",
    "rewards_history = []\n",
    "entropy_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please pay extra attention to how we scale rewards in training. We do that for multiple reasons.\n",
    "\n",
    "1. All rewards are multiples of 100, and even an untrained agent can get a score of 800. Therefore, even in the beginning of training, the critic will have to predict pretty large numbers. Neural networks require extra tinkering to output large numbers reliably. In this case, the easiest workaround is just to scale back those numbers.\n",
    "2. We have already tweaked the hyperparameters (loss coefficients) to work well with this scaling.\n",
    "\n",
    "Please note however that we would not have needed to do this in plain REINFORCE without entropy regularization but with Adam optimizer.\n",
    "\n",
    "In REINFORCE, there is only actor and no critic. Without entropy regularization, actor loss is just policy gradient. It is proportional to rewards, but it only affects the scale of the gradient. However, Adam maintains a running average of the variance of the gradient for each parameter it optimizes, and normalizes the gradient by its variance in each optimization step. This will negate any scaling of the gradient.\n",
    "\n",
    "If your implementation works correctly, you can comment out the `batch_rewards = batch_rewards * 0.01` line, restart training, and see it explode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "with tqdm.trange(len(entropy_history), 100000) as t:\n",
    "    for i in t:\n",
    "        agent_outputs = agent.step(batch_states)\n",
    "        batch_actions = agent.sample_actions(agent_outputs)\n",
    "        batch_next_states, batch_rewards, batch_done, _ = env_batch.step(batch_actions)\n",
    "\n",
    "        # Reward scaling. See above for explanation.\n",
    "        batch_rewards = batch_rewards * 0.01\n",
    "\n",
    "        feed_dict = {\n",
    "            states_ph: batch_states,\n",
    "            actions_ph: batch_actions,\n",
    "            next_states_ph: batch_next_states,\n",
    "            rewards_ph: batch_rewards,\n",
    "            is_done_ph: batch_done,\n",
    "        }\n",
    "\n",
    "        _, ent_t = sess.run([train_step, entropy], feed_dict)\n",
    "        entropy_history.append(np.mean(ent_t))\n",
    "\n",
    "        batch_states = batch_next_states\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            if i % 2500 == 0:\n",
    "                rewards_history.append(np.mean(evaluate(agent, env, n_games=3)))\n",
    "                if rewards_history[-1] >= 5000:\n",
    "                    print(\"Your agent has earned the yellow belt\")\n",
    "\n",
    "            clear_output(True)\n",
    "\n",
    "            plt.figure(figsize=[8, 4])\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(rewards_history, label='rewards')\n",
    "            plt.plot(ewma(np.array(rewards_history), span=10), marker='.', label='rewards ewma@10')\n",
    "            plt.title(\"Session rewards\")\n",
    "            plt.grid()\n",
    "            plt.legend()\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(entropy_history, label='entropy')\n",
    "            plt.plot(ewma(np.array(entropy_history), span=1000), label='entropy ewma@1000')\n",
    "            plt.title(\"Policy entropy\")\n",
    "            plt.grid()\n",
    "            plt.legend()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relax and grab some refreshments while your agent is locked in an infinite loop of violence and death.\n",
    "\n",
    "__How to interpret plots:__\n",
    "\n",
    "The session reward is the easy thing: it should in general go up over time, but it's okay if it fluctuates ~~like crazy~~. It's also OK if it doesn't increase substantially before some 10-20k initial steps, and some people who tried this assignment [told us](https://www.coursera.org/learn/practical-rl/discussions/all/threads/3OnFNVxEEemLZA644RFX2A) they didn't see improvements until around 60k steps. However, if reward reaches zero and doesn't seem to get up over 2-3 evaluations, there's something wrong happening.\n",
    "\n",
    "Since we use a policy-based method, we also keep track of __policy entropy__ — the same one you used as a regularizer. The only important thing about it is that your entropy shouldn't drop too low (`< 0.1`) before your agent gets the yellow belt. Or at least it can drop there, but _it shouldn't stay there for long_.\n",
    "\n",
    "If it does, the culprit is likely:\n",
    "* Some bug in entropy computation. Remember that it is $-\\sum p(a_i) \\cdot \\log p(a_i)$.\n",
    "* Your model architecture is broken in some way: for example, if you create layers in `Agent.symbolic_step()` rather than in `Agent.__init__()`, then effectively you will be training two separate agents: one for `logits, state_values` and another one for `next_logits, next_state_values`.\n",
    "* Your architecture is different from the one we suggest and it converges too quickly. Change your architecture or increase entropy coefficient in actor loss. \n",
    "* Gradient explosion: just [clip gradients](https://stackoverflow.com/a/43486487) and maybe use a smaller network\n",
    "* Us. Or TF developers. Or aliens. Or lizardfolk. Contact us on forums before it's too late!\n",
    "\n",
    "If you're debugging, just run `logits, values = agent.step(batch_states)` and manually look into logits and values. This will reveal the problem 9 times out of 10: you'll likely see some NaNs or insanely large numbers or zeros. Try to catch the moment when this happens for the first time and investigate from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Final\" evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym.wrappers\n",
    "\n",
    "with gym.wrappers.Monitor(make_env(), directory=\"videos\", force=True) as env_monitor:\n",
    "    final_rewards = evaluate(agent, env_monitor, n_games=3)\n",
    "\n",
    "print(\"Final mean reward:\", np.mean(final_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show video. This may not work in some setups. If it doesn't\n",
    "# work for you, you can download the videos and view them locally.\n",
    "\n",
    "from pathlib import Path\n",
    "from IPython.display import HTML\n",
    "\n",
    "video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(video_names[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(video_names[-2]))  # You can also try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't see videos above, just navigate to `./videos` and download `.mp4` files from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from submit import submit_kungfu\n",
    "env = make_env()\n",
    "submit_kungfu(agent, env, evaluate, <EMAIL>, <TOKEN>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now what?\n",
    "Well, 5k reward is [just the beginning](https://www.buzzfeed.com/mattjayyoung/what-the-color-of-your-karate-belt-actually-means-lg3g). Can you get past 200? With recurrent neural network memory, chances are you can even beat 400!\n",
    "\n",
    "* Try n-step advantage and \"lambda\"-advantage (aka GAE) - see [this article](https://arxiv.org/abs/1506.02438)\n",
    " * This change should improve early convergence a lot\n",
    "* Try recurrent neural network \n",
    " * RNN memory will slow things down initially, but in will reach better final reward at this game\n",
    "* Implement asynchronuous version\n",
    " * Remember [A3C](https://arxiv.org/abs/1602.01783)? The first \"A\" stands for asynchronuous. It means there are several parallel actor-learners out there.\n",
    " * You can write custom code for synchronization, but we recommend using [redis](https://redis.io/)\n",
    "   * You can store full parameter set in redis, along with any other metadate\n",
    "   * Here's a _quick_ way to (de)serialize parameters for redis\n",
    "   ```\n",
    "   import joblib\n",
    "   from six import BytesIO\n",
    "```\n",
    "```\n",
    "   def dumps(data):\n",
    "        \"converts whatever to string\"\n",
    "        s = BytesIO()\n",
    "        joblib.dump(data,s)\n",
    "        return s.getvalue()\n",
    "``` \n",
    "```\n",
    "    def loads(string):\n",
    "        \"converts string to whatever was dumps'ed in it\"\n",
    "        return joblib.load(BytesIO(string))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
